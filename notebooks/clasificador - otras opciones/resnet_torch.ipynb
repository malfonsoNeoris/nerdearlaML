{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trucks task with ['cemento', 'no_truck', 'otro', 'plataforma', 'volteo', 'volteo_corto'] categories defined\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from dataset_trucks import ImageClassificationDataset\n",
    "\n",
    "TASK = 'trucks'\n",
    "# TASK = 'emotions'\n",
    "# TASK = 'fingers'\n",
    "# TASK = 'diy'\n",
    "\n",
    "CATEGORIES = ['cemento', 'no_truck','otro','plataforma','volteo','volteo_corto']\n",
    "# CATEGORIES = ['none', 'happy', 'sad', 'angry']\n",
    "# CATEGORIES = ['1', '2', '3', '4', '5']\n",
    "# CATEGORIES = [ 'diy_1', 'diy_2', 'diy_3']\n",
    "\n",
    "DATASETS = ['A']\n",
    "# DATASETS = ['A', 'B', 'C']\n",
    "\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "datasets = {}\n",
    "for name in DATASETS:\n",
    "    datasets[name] = ImageClassificationDataset('./data', CATEGORIES, TRANSFORMS)\n",
    "    \n",
    "print(\"{} task with {} categories defined\".format(TASK, CATEGORIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path):\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model configured and model_widget created\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# ALEXNET\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "# model.classifier[-1] = torch.nn.Linear(4096, len(dataset.categories))\n",
    "\n",
    "# SQUEEZENET \n",
    "# model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "# model.classifier[1] = torch.nn.Conv2d(512, len(dataset.categories), kernel_size=1)\n",
    "# model.num_classes = len(dataset.categories)\n",
    "\n",
    "# RESNET 18\n",
    "# model = torchvision.models.resnet18(pretrained=True)\n",
    "# model.fc = torch.nn.Linear(512, len(dataset.categories))\n",
    "categories = 6\n",
    "# RESNET 34\n",
    "model = torchvision.models.resnet34(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, categories)\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "# display(model_widget)\n",
    "print(\"model configured and model_widget created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[DATASETS[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import threading\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "epochs_widget = ipywidgets.IntText(description='epochs', value=1)\n",
    "eval_button = ipywidgets.Button(description='evaluate')\n",
    "train_button = ipywidgets.Button(description='train')\n",
    "loss_widget = ipywidgets.FloatText(description='loss')\n",
    "accuracy_widget = ipywidgets.FloatText(description='accuracy')\n",
    "progress_widget = ipywidgets.FloatProgress(min=0.0, max=1.0, description='progress')\n",
    "\n",
    "def train_eval(is_training):\n",
    "    global BATCH_SIZE, LEARNING_RATE, MOMENTUM, model, dataset, optimizer,epochs_widget, accuracy_widget, loss_widget, progress_widget\n",
    "    \n",
    "    try:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        if is_training:\n",
    "            model = model.train()\n",
    "        else:\n",
    "            model = model.eval()\n",
    "        while epochs_widget.value > 0:\n",
    "            i = 0\n",
    "            sum_loss = 0.0\n",
    "            error_count = 0.0\n",
    "            for images, labels in iter(train_loader):\n",
    "                # send data to device\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                if is_training:\n",
    "                    # zero gradients of parameters\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # execute model to get outputs\n",
    "                outputs = model(images)\n",
    "\n",
    "                # compute loss\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "                if is_training:\n",
    "                    # run backpropogation to accumulate gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # step optimizer to adjust parameters\n",
    "                    optimizer.step()\n",
    "\n",
    "                # increment progress\n",
    "                error_count += len(torch.nonzero(outputs.argmax(1) - labels).flatten())\n",
    "                count = len(labels.flatten())\n",
    "                i += count\n",
    "                sum_loss += float(loss)\n",
    "                progress_widget.value = i / len(dataset)\n",
    "                loss_widget.value = sum_loss / i\n",
    "                accuracy_widget.value = 1.0 - error_count / i\n",
    "                \n",
    "            if is_training:\n",
    "                epochs_widget.value = epochs_widget.value - 1\n",
    "            else:\n",
    "                break\n",
    "    except e:\n",
    "        pass\n",
    "    model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(VBox(children=(IntText(value=1, description='epochs'), FloatProgress(value=0.0, description='prâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e47aed95a9fd4638b4d1350809d1c78c"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "\n",
    "train_button.on_click(lambda c: train_eval(is_training=True))\n",
    "eval_button.on_click(lambda c: train_eval(is_training=False))\n",
    "    \n",
    "train_eval_widget = ipywidgets.VBox([\n",
    "    epochs_widget,\n",
    "    progress_widget,\n",
    "    loss_widget,\n",
    "    accuracy_widget,\n",
    "    ipywidgets.HBox([train_button, eval_button])\n",
    "])\n",
    "# Combine all the widgets into one display\n",
    "all_widget = ipywidgets.VBox([ \n",
    "    train_eval_widget,\n",
    "])\n",
    "\n",
    "display(all_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('truck_classifier_resnet34_torch_100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TruckClassifierTorch():\n",
    "    def __init__(self,categories,model_path):\n",
    "        self.device = torch.device('cuda')\n",
    "        self.mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "        self.std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "        self.categories = categories\n",
    "        \n",
    "        self.model = torchvision.models.resnet34(pretrained=False)\n",
    "        self.model.fc = torch.nn.Linear(512, self.categories)\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        # ALEXNET\n",
    "        # model = torchvision.models.alexnet(pretrained=True)\n",
    "        # model.classifier[-1] = torch.nn.Linear(4096, len(dataset.categories))\n",
    "\n",
    "        # SQUEEZENET \n",
    "        # model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "        # model.classifier[1] = torch.nn.Conv2d(512, len(dataset.categories), kernel_size=1)\n",
    "        # model.num_classes = len(dataset.categories)\n",
    "\n",
    "        # RESNET 18\n",
    "        # model = torchvision.models.resnet18(pretrained=True)\n",
    "        # model.fc = torch.nn.Linear(512, len(dataset.categories))\n",
    "\n",
    "    def preprocess(self,image):\n",
    "        image = PIL.Image.fromarray(image)\n",
    "        image = transforms.functional.to_tensor(image).to(self.device)\n",
    "        image.sub_(self.mean[:, None, None]).div_(self.std[:, None, None])\n",
    "        return image[None, ...]\n",
    "        \n",
    "    def detect(self,image):\n",
    "        # Inference\n",
    "        preprocessed = self.preprocess(image)\n",
    "        output = self.model(preprocessed)\n",
    "        output = F.softmax(output, dim=1).detach().cpu().numpy().flatten()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.12155704 0.00204893 0.05717224 0.00131879 0.81596553 0.00193751]\nvolteo\n"
     ]
    }
   ],
   "source": [
    "tc = TruckClassifierTorch(6,'truck_classifier_resnet34_torch.pth')\n",
    "img = cv2.imread('exit_gate_20210619070712_5ab48baa-68fc-4a8e-9f69-6047ffca612f_TRUCK.jpg')\n",
    "\n",
    "output = tc.detect(img)\n",
    "print(output)\n",
    "print(CATEGORIES[np.argmax(output)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['cemento', 'no_truck', 'otro', 'plataforma', 'volteo', 'volteo_corto']"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "volteo_corto 190\nno_truck 323\notro 214\ncemento 532\nplataforma 53\nvolteo 995\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in os.listdir('data'):\n",
    "    print(i,len(os.listdir(f'data/{i}')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}